{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primetals Python implementation of cifar10 Geometrically Inspired Kernel Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from func import divide_data_into_non_iid_label_screw, Classifier, predictionClassifier\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data of the cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_and_labels(file_paths, prefix=\"._\"):\n",
    "    \"\"\"\n",
    "    Load ResNet-50 features and labels from given file paths.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of image file paths.\n",
    "        prefix: Prefix for ignoring certain files.\n",
    "    \n",
    "    Returns:\n",
    "        features: A NumPy array of shape (2048, num_files) with ResNet-50 features.\n",
    "        labels: A NumPy array containing the labels.\n",
    "    \"\"\"\n",
    "    num_files = len(file_paths)\n",
    "    features = np.zeros((2048, num_files))\n",
    "    labels = []\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "        if not base_name.startswith(prefix):\n",
    "            mat_file = os.path.join(os.path.dirname(file_path), f'{base_name}_resnet50.mat')\n",
    "            if os.path.exists(mat_file):\n",
    "                resnet50_features = scipy.io.loadmat(mat_file)['resnet50_features']\n",
    "                features[:, i] = resnet50_features.flatten()\n",
    "            else:\n",
    "                print(f\"Missing ResNet-50 features for {file_path}\")\n",
    "            \n",
    "            # Assuming labels are part of the file path (e.g., folder names are classes)\n",
    "            # Extract label from the folder name\n",
    "            label = int(os.path.basename(os.path.dirname(file_path)))\n",
    "            labels.append(label)\n",
    "\n",
    "    # Apply tanh activation to the features\n",
    "    features = np.tanh(features)\n",
    "    return features, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 10 classes.\n",
      "Found 10000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4232)\n",
    "\n",
    "    # Define the root folder\n",
    "dataset_folder = os.path.join(os.getcwd().replace('GIKM_python', 'Datasets'), 'cifar10')\n",
    "\n",
    "# Load training and testing images (assuming folders 'train' and 'test')\n",
    "training_dataset = image_dataset_from_directory(os.path.join(dataset_folder, 'train'),\n",
    "                                                label_mode='int', image_size=(224, 224), shuffle=False)\n",
    "testing_dataset = image_dataset_from_directory(os.path.join(dataset_folder, 'test'),\n",
    "                                                label_mode='int', image_size=(224, 224), shuffle=False)\n",
    "\n",
    "# Initialize the feature matrix for training data\n",
    "num_training_files = len(training_dataset.file_paths)\n",
    "y_data_trn = np.zeros((2048, num_training_files))\n",
    "prefix = \"._\"\n",
    "# Load features for training data\n",
    "for i, file_path in enumerate(training_dataset.file_paths):\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    if not base_name.startswith(prefix):\n",
    "        mat_file = os.path.join(os.path.dirname(file_path), f'{base_name}_resnet50.mat')\n",
    "        #print(f'Reading feature of file = {file_path}')\n",
    "        resnet50_features = scipy.io.loadmat(mat_file)['resnet50_features']\n",
    "        y_data_trn[:, i] = resnet50_features.flatten()\n",
    "\n",
    "# Apply tanh activation\n",
    "y_data_trn = np.tanh(y_data_trn)\n",
    "\n",
    "# Get training labels\n",
    "training_labels = np.concatenate([labels for _, labels in training_dataset], axis=0)\n",
    "classes = np.unique(training_labels)\n",
    "C = len(classes)\n",
    "\n",
    "# Initialize the feature matrix for testing data\n",
    "num_testing_files = len(testing_dataset.file_paths)\n",
    "y_data_test = np.zeros((2048, num_testing_files))\n",
    "labels_trn = training_labels\n",
    "# Load features for testing data\n",
    "for i, file_path in enumerate(testing_dataset.file_paths):\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    mat_file = os.path.join(os.path.dirname(file_path), f'{base_name}_resnet50.mat')\n",
    "    #print(f'Reading feature of file = {file_path}')\n",
    "    resnet50_features = scipy.io.loadmat(mat_file)['resnet50_features']\n",
    "    y_data_test[:, i] = resnet50_features.flatten()\n",
    "\n",
    "# Apply tanh activation\n",
    "y_data_test = np.tanh(y_data_test)\n",
    "\n",
    "# Get testing labels\n",
    "testing_labels = np.concatenate([labels for _, labels in testing_dataset], axis=0)\n",
    "\n",
    "# Map testing labels to indices\n",
    "labels_test = testing_labels\n",
    "\n",
    "n_clients = 100\n",
    "n_experiments = 3\n",
    "avg_local_acc_arr = np.zeros(n_experiments)\n",
    "avg_global_acc_arr = np.zeros(n_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall maximum modeling error = 0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m classes_client \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels_trn[client_id_trn \u001b[38;5;241m==\u001b[39m j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Select the test data belonging to the classes present in this client\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m test_data_ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclasses_client\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m y_data_test_client \u001b[38;5;241m=\u001b[39m y_data_test[:, test_data_ind]\n\u001b[0;32m     13\u001b[0m labels_test_client \u001b[38;5;241m=\u001b[39m labels_test[test_data_ind]\n",
      "File \u001b[1;32mc:\\Users\\brucker\\Documents\\projects\\gikm\\gikm\\venv\\lib\\site-packages\\numpy\\core\\shape_base.py:359\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "for k in range(n_experiments):\n",
    "    client_id_trn = divide_data_into_non_iid_label_screw(labels_trn, n_clients, round(0.2 * C))\n",
    "    local_acc_arr = np.zeros(n_clients)\n",
    "    distance_matrix = np.full((C, y_data_test.shape[1]), np.inf)\n",
    "\n",
    "    for j in range(n_clients):\n",
    "        clf = Classifier(y_data_trn[:, client_id_trn == j+1], labels_trn[client_id_trn == j+1], 20, 1000)\n",
    "        classes_client = np.unique(labels_trn[client_id_trn == j+1])\n",
    "\n",
    "        # Select the test data belonging to the classes present in this client\n",
    "        test_data_ind = np.hstack([np.where(labels_test == cls)[0] for cls in classes_client])\n",
    "        y_data_test_client = y_data_test[:, test_data_ind]\n",
    "        labels_test_client = labels_test[test_data_ind]\n",
    "\n",
    "        distance_arr, labels_predicted = predictionClassifier(y_data_test_client, clf)\n",
    "        local_acc_arr[j] = np.mean(labels_predicted == labels_test_client)\n",
    "\n",
    "        for i, test_ind in enumerate(test_data_ind):\n",
    "            distance_matrix[labels_predicted[i], test_ind] = min(distance_arr[i], distance_matrix[labels_predicted[i], test_ind])\n",
    "\n",
    "    # Determine global predictions\n",
    "    min_distance = np.min(distance_matrix, axis=0)\n",
    "    hat_labels_test = np.zeros(y_data_test.shape[1], dtype=int)\n",
    "    for i in range(C):\n",
    "        hat_labels_test[distance_matrix[i, :] == min_distance] = i# + 1\n",
    "\n",
    "    # Global accuracy calculation\n",
    "    global_acc_arr = np.zeros(n_clients)\n",
    "    for j in range(n_clients):\n",
    "        classes_client = np.unique(labels_trn[client_id_trn == j+1])\n",
    "        test_data_ind = np.hstack([np.where(labels_test == cls)[0] for cls in classes_client])\n",
    "        global_acc_arr[j] = np.mean(hat_labels_test[test_data_ind] == labels_test[test_data_ind])\n",
    "\n",
    "    avg_local_acc_arr[k] = np.mean(local_acc_arr)\n",
    "    avg_global_acc_arr[k] = np.mean(global_acc_arr)\n",
    "\n",
    "# Compute final results\n",
    "mean_local_acc_20 = np.mean(avg_local_acc_arr)\n",
    "std_local_acc_20 = np.std(avg_local_acc_arr)\n",
    "mean_global_acc_20 = np.mean(avg_global_acc_arr)\n",
    "std_global_acc_20 = np.std(avg_global_acc_arr)\n",
    "\n",
    "print(f\"Local accuracy (20%): {mean_local_acc_20:.6f}, std = {std_local_acc_20:.6f}\")\n",
    "print(f\"Global accuracy (20%): {mean_global_acc_20:.6f}, std = {std_global_acc_20:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
