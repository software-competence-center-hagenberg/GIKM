{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primetals Python implementation of mnist Geometrically Inspired Kernel Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from func import Classifier, predictionClassifier, combineMultipleClassifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data for mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the file path for the .mat file\n",
    "# Assuming the current working directory is the Python equivalent of MATLAB's pwd\n",
    "print(os.getcwd())\n",
    "dataset_path = os.path.join(os.getcwd().replace('GIKM_python', 'Datasets'), 'MNIST', 'mnist_all.mat')\n",
    "\n",
    "# Load the .mat file\n",
    "data = scipy.io.loadmat(dataset_path)\n",
    "\n",
    "# Extract and convert train and test data to double (float) and normalize\n",
    "train0 = data['train0']\n",
    "train1 = data['train1']\n",
    "train2 = data['train2']\n",
    "train3 = data['train3']\n",
    "train4 = data['train4']\n",
    "train5 = data['train5']\n",
    "train6 = data['train6']\n",
    "train7 = data['train7']\n",
    "train8 = data['train8']\n",
    "train9 = data['train9']\n",
    "\n",
    "test0 = data['test0']\n",
    "test1 = data['test1']\n",
    "test2 = data['test2']\n",
    "test3 = data['test3']\n",
    "test4 = data['test4']\n",
    "test5 = data['test5']\n",
    "test6 = data['test6']\n",
    "test7 = data['test7']\n",
    "test8 = data['test8']\n",
    "test9 = data['test9']\n",
    "\n",
    "# Concatenate all training data and normalize by dividing by 255\n",
    "y_data_trn = np.hstack([\n",
    "    train0.T, train1.T, train2.T, train3.T, train4.T,\n",
    "    train5.T, train6.T, train7.T, train8.T, train9.T\n",
    "]).astype(np.float64) / 255.0\n",
    "\n",
    "# Print the shape of the training data for verification\n",
    "print(f\"shape of train0: {train0.shape}\")\n",
    "print(\"Shape of y_data_trn:\", y_data_trn.shape)\n",
    "\n",
    "# Similarly, load test data if needed\n",
    "# test0 = data['test0']\n",
    "# test1 = data['test1']\n",
    "# ...\n",
    "# and concatenate them as needed\n",
    "y_data_trn = np.tanh(y_data_trn)\n",
    "\n",
    "# Create labels for training data\n",
    "labels_trn = np.hstack([\n",
    "    1 * np.ones(train0.shape[0]),\n",
    "    2 * np.ones(train1.shape[0]),\n",
    "    3 * np.ones(train2.shape[0]),\n",
    "    4 * np.ones(train3.shape[0]),\n",
    "    5 * np.ones(train4.shape[0]),\n",
    "    6 * np.ones(train5.shape[0]),\n",
    "    7 * np.ones(train6.shape[0]),\n",
    "    8 * np.ones(train7.shape[0]),\n",
    "    9 * np.ones(train8.shape[0]),\n",
    "    10 * np.ones(train9.shape[0])\n",
    "]).astype(int)\n",
    "\n",
    "# Convert test data to double (float64) and normalize\n",
    "y_data_test = np.hstack([\n",
    "    test0.T, test1.T, test2.T, test3.T, test4.T,\n",
    "    test5.T, test6.T, test7.T, test8.T, test9.T\n",
    "]).astype(np.float64) / 255.0\n",
    "\n",
    "# Apply tanh activation function\n",
    "y_data_test = np.tanh(y_data_test)\n",
    "\n",
    "# Create labels for test data\n",
    "labels_test = np.hstack([\n",
    "    1 * np.ones(test0.shape[0]),\n",
    "    2 * np.ones(test1.shape[0]),\n",
    "    3 * np.ones(test2.shape[0]),\n",
    "    4 * np.ones(test3.shape[0]),\n",
    "    5 * np.ones(test4.shape[0]),\n",
    "    6 * np.ones(test5.shape[0]),\n",
    "    7 * np.ones(test6.shape[0]),\n",
    "    8 * np.ones(test7.shape[0]),\n",
    "    9 * np.ones(test8.shape[0]),\n",
    "    10 * np.ones(test9.shape[0])\n",
    "]).astype(int)\n",
    "\n",
    "# Unique labels and number of labels\n",
    "labels = np.unique(labels_trn)\n",
    "Q = len(labels)\n",
    "\n",
    "# Initialize lists to store results for each class\n",
    "min_distance_arr = [None] * Q\n",
    "labels_arr_arr = [None] * Q\n",
    "max_modeling_error = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Q):\n",
    "    # Find indices for the current label\n",
    "    ind = np.where(labels_trn == labels[i])[0]\n",
    "\n",
    "    # Train the classifier for the current class data\n",
    "    CLF = Classifier(y_data_trn[:, ind], labels_trn[ind], subspace_dim=20, Nb=1000)\n",
    "\n",
    "    # Perform prediction on the test data\n",
    "    min_distance_arr[i], labels_arr_arr[i] = predictionClassifier(y_data_test, CLF)\n",
    "\n",
    "    # Track the maximum modeling error encountered\n",
    "    max_modeling_error = max(max_modeling_error, CLF['max_modeling_error'])\n",
    "\n",
    "# Combine results from multiple classifiers\n",
    "_, hat_labels_test = combineMultipleClassifiers(min_distance_arr, labels_arr_arr)\n",
    "\n",
    "# Calculate accuracy of the predictions\n",
    "acc = np.mean(hat_labels_test == labels_test)\n",
    "\n",
    "print(f\"Overall maximum modeling error: {max_modeling_error}\")\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Rundown\n",
    "- For every **label** in labels:\n",
    "  - find all **ind** (indexes) in the train set belonging to the class **label**\n",
    "  - create a Classifier **CLF** with the data (X,y) belonging to class **label** with subspace dim 20 and Nb defining the number of cluster/subsets\n",
    "    - for every unique class label define a separate **autoencoder** with function **parallelAutoencoders**\n",
    "      - Depending on the ratio between Number of samples **N** and subset size **Nb**\n",
    "      - **if** more than one cluster\n",
    "        - cluster the data with **Kmeans**\n",
    "        - eliminate clusters with less than two samples in it\n",
    "        - Reducing the Number of Clusters **(While Loop)**\n",
    "        - find clusters with less than 30 samples, repeat process from reducing clusters\n",
    "        - per cluster train an **Autoencoder** with function **parallel Autoencoder**\n",
    "          - Reduce data dimensionality and project data onto new dimensions with **dimReduce** and return **y_data_subspace** and **PC** (Principal components)\n",
    "          - if value has not enough significant spread within data, reduce dimensionality by one\n",
    "          - compute the weights matrix with the inverse of the covariance matrix\n",
    "          - calc **KxxMatrix** with x matrix, weights matrix and kerneltype (so far only gaussian)\n",
    "            - define size of Kxx matrix with dimensions of x matrix\n",
    "            - calculate distance between all samples and calculate squared distance with the weights matrix\n",
    "            - apply gaussian filter with $ \\exp(-(0.5/n)* \\text{Kxx})$\n",
    "          - calculate **kernel least squares** with **KxxMatrix** and **ydata**\n",
    "            - regularize data and control overfit with lambda\n",
    "            - run multiple regulations to improve accuracy\n",
    "          - apply **AutoencoderFiltering** to project data to autoencoders\n",
    "            - project data into autoencoder subspace\n",
    "            - handle large number of samples in batches\n",
    "            - for each batch, kernel based weight calculation\n",
    "            - Calculate distance between new projected data and train data\n",
    "  - define **PredictionClassifier** with **Classifier** and **y_data_test** input\n",
    "    - for each class label calculate the distance for every data point with the specific autoencoder to the label\n",
    "      - apply **AutoencoderFiltering** to calculate the distances\n",
    "    - predicted label depends on the minimum distance to the class within the distance_matrix for every label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
