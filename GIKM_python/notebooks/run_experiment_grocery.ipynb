{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primetals Python implementation of grocery Geometrically Inspired Kernel Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from func import combineMultipleClassifiers, Classifier, predictionClassifier\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(4232)\n",
    "images_folder = os.path.join(os.getcwd(),'Datasets', 'FreiburgGrocery')\n",
    "    # Get the class names from the folder structure (i.e., subdirectories of 'FreiburgGrocery')\n",
    "classes = [d for d in os.listdir(images_folder) if os.path.isdir(os.path.join(images_folder, d))]\n",
    "\n",
    "# Read train and test datasets from the txt files\n",
    "train_df = pd.read_csv('GIKM_python/trainGrocery.txt', delimiter=' ', header=None)\n",
    "test_df = pd.read_csv('GIKM_python/testGrocery.txt', delimiter=' ', header=None)\n",
    "\n",
    "# Map training and testing file names to their full paths\n",
    "train_files = [os.path.join(images_folder, fname) for fname in train_df[0]]\n",
    "test_files = [os.path.join(images_folder, fname) for fname in test_df[0]]\n",
    "\n",
    "# Initialize arrays for training data\n",
    "y_data_trn = np.zeros((2048, len(train_files)))\n",
    "labels_trn = np.zeros((len(train_files),), dtype=int)\n",
    "\n",
    "for i, filepath in enumerate(train_files):\n",
    "    print(f\"Reading feature of file = {filepath}\")\n",
    "    \n",
    "    # Load resnet50 features from the .mat file\n",
    "    file = filepath.replace(\"/\",\"\\\\\")\n",
    "    mat = loadmat(file.replace(\".png\",'_resnet50.mat'))\n",
    "    resnet50_features = mat['resnet50_features']\n",
    "    \n",
    "    y_data_trn[:, i] = resnet50_features.flatten()\n",
    "    \n",
    "    # Get the class label based on the parent directory name\n",
    "    class_name = os.path.basename(os.path.dirname(filepath))\n",
    "    labels_trn[i] = classes.index(class_name)\n",
    "\n",
    "y_data_trn = np.tanh(y_data_trn)\n",
    "\n",
    "\n",
    "# Initialize arrays for testing data\n",
    "y_data_test = np.zeros((2048, len(test_files)))\n",
    "labels_test = np.zeros((len(test_files),), dtype=int)\n",
    "for i, filepath in enumerate(test_files):\n",
    "    print(f\"Reading feature of file = {filepath}\")\n",
    "    \n",
    "    # Load resnet50 features from the .mat file\n",
    "    file = filepath.replace(\"/\",\"\\\\\")\n",
    "    mat = loadmat(file.replace(\".png\",'_resnet50.mat'))\n",
    "    resnet50_features = mat['resnet50_features']\n",
    "    \n",
    "    y_data_test[:, i] = resnet50_features.flatten()\n",
    "    \n",
    "    # Get the class label based on the parent directory name\n",
    "    class_name = os.path.basename(os.path.dirname(filepath))\n",
    "    labels_test[i] = classes.index(class_name)\n",
    "\n",
    "# Apply tanh to testing data\n",
    "y_data_test = np.tanh(y_data_test)\n",
    "\n",
    "# Simulate number of clients as number of classes\n",
    "unique_labels = np.unique(labels_trn)\n",
    "Q = len(unique_labels)\n",
    "min_distance_arr = [None] * Q\n",
    "labels_arr_arr = [None] * Q\n",
    "max_modeling_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Q):\n",
    "    ind = np.where(labels_trn == unique_labels[i])[0]\n",
    "    \n",
    "    clf = Classifier(y_data_trn[:, ind], labels_trn[ind], 20, 1000)\n",
    "    min_distance_arr[i], labels_arr_arr[i] = predictionClassifier(y_data_test, clf)\n",
    "    \n",
    "    max_modeling_error = max(max_modeling_error, clf[\"max_modeling_error\"])\n",
    "\n",
    "_, hat_labels_test = combineMultipleClassifiers(min_distance_arr, labels_arr_arr)\n",
    "\n",
    "# Calculate global accuracy\n",
    "acc = np.mean(hat_labels_test == labels_test)\n",
    "print(f\"Global accuracy = {acc}, Maximum modeling error = {max_modeling_error}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
